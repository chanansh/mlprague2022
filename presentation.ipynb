{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Reinforcement learning & contextual bandits\n",
    "- Petr Stanislav & Michal Kubi≈°ta\n",
    "- [dataclair.ai](https://dataclair.ai/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<img src=\"www/dcl_logo.jpg\" align=\"middle\" width=\"600\" height=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Structure\n",
    "- Intro to reinforcement learning\n",
    "- Multi-armed bandits\n",
    "- Contextual bandits\n",
    "- TF-Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Intro to Reinforcement Learning (RL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why do we care?\n",
    "**assumption:** we have mastered all aspects of supervised learning for any kind of data type (text, images, tabular, ...)\n",
    "\n",
    "\n",
    "**problem:** build a purely ML system to play perfect chess (using supervised learning)  \n",
    "**input:** picture of chessboard  \n",
    "**output:** which move to make  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This should be easy, right?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Problems?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1. **what's the best move?**\n",
    "<img src=\"www/best_move.png\" align=\"middle\" width=\"300\" height=\"300\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Problems?\n",
    "1. **what's the best move?**\n",
    "1. **time(step)-dependency**\n",
    "<img src=\"www/trebuchet.png\" align=\"middle\" width=\"300\" height=\"300\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Problems?\n",
    "1. **what's the best move?**\n",
    "1. **time(step)-dependency**\n",
    "1. **training data**\n",
    "<img src=\"www/chessboard.png\" align=\"middle\" width=\"300\" height=\"300\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Solutions?\n",
    "1. **what's the best move?**\n",
    "    - what if we instead assign specific rewards for different outcomes?\n",
    "    - chess example:\n",
    "        - if we take a piece &#8594; value of that piece (1, 3, 5, 9)\n",
    "        - if we win &#8594; 30\n",
    "        - otherwise &#8594; 0\n",
    "1. time(step)-dependency\n",
    "1. training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why reward? (problem 1)\n",
    "- we don't know the \"true\" result\n",
    "- but we can define a general process to evaluate the actions we take\n",
    "    - chess - taking a piece, winning the game\n",
    "    - autonomous car - moving in the right direction, not causing any crashes\n",
    "- we want to maximise the sum of all received rewards (**total reward**)\n",
    "    - $TR_T = \\sum_{t=1}^{T}r_t$\n",
    "- `value` vs `reward`\n",
    "    - when taking the action $a$ in state $s$:\n",
    "        - `reward` is the observed immediate feedback\n",
    "        - `value` is predicted long-term potential\n",
    "            - an approximation of the **total reward**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Is reward enough? (problem 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](www/reward_is_enough.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Limitation of reward (problem 1)\n",
    "- credit assignment\n",
    "    - how to match the future rewards to the actions that lead to them\n",
    "    - e.g. we win a chess game (large positive reward), now we need to credit the actions that has helped us the most\n",
    "    - let's review two chess positions:\n",
    "        - [scholar's mate](https://lichess.org/analysis)\n",
    "        - [back rank mate](https://lichess.org/analysis/k1rq2nr/pp4pp/3p4/8/8/3P4/PPQ3PP/K1R3NR_w_Kk_-_0_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- [perverse incentive](https://en.wikipedia.org/wiki/Perverse_incentive) (cobra effect)\n",
    "    - [The Surprising Creativity of Digital Evolution](https://arxiv.org/abs/1803.03453)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## [HalfCheetah](https://www.alexirpan.com/2018/02/14/rl-hard.html)  (problem 1)\n",
    "\n",
    "\n",
    "<video width=\"960\" height=\"480\" controls>\n",
    "<source src=\"https://www.alexirpan.com/public/rl-hard/upsidedown_half_cheetah.mp4\" type=\"video/mp4\">\n",
    "</video>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Solutions?\n",
    "1. what's the best move?\n",
    "1. time(step)-dependency\n",
    "1. **training data**\n",
    "    - what if we let the algorithm generate it's own data?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Generating the training dataset (problem 3)\n",
    "- what we need\n",
    "    - some **model** capable of online learning\n",
    "        - get **input**, provide **decission**\n",
    "    - some **engine** that will:\n",
    "        - provide **input** (observation)\n",
    "        - calculate the **reward** based on the model **decission** (prediction)\n",
    "- we collect the data\n",
    "    - have the engine generate the **inputs**\n",
    "    - get the model to predict (randomly) the **decission**\n",
    "    - collect the **reward** for each **decission**\n",
    "    - save all of it - `(input, decission, reward)`\n",
    "- after we collect enough data we use it to train the **model** and go collect more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## With RL terms (problem 3)\n",
    "- we need some simulation - **environment**\n",
    "    - includes definition of possible `actions`\n",
    "    - chess engine with basic rules, or physical engine for robots\n",
    "- the **enviroment** produces some observation - `state`\n",
    "    - e.g. position of all pieces\n",
    "- we need model that can decide based on the `observation` - **agent**\n",
    "    - oracle - regression model that predicts the `value` of each `action` based on `observation`\n",
    "    - policy - decide on the `action` based on predictions from oracle\n",
    "- the agent plays the action and receives `reward` and a new `state` from the **environment**\n",
    "\n",
    "\n",
    "<img src=\"www/RL_feedback_loop.jpg\" align=\"middle\" width=\"400\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why separate oracle and policy? (problem 3)\n",
    "- we start with sequential classification\n",
    "- for some reason turn it into a regression\n",
    "- and attach some decission process on top of it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Exploration vs Exploitation\n",
    "- we have (some degree of) control over how our dataset will look like\n",
    "- this let's us decide between:\n",
    "    - exploration - let the policy select sub-optimal action to see what happens\n",
    "    - exploitation - make the policy select the action with highest predicted reward\n",
    "- the task of the policy is to try to find the optimal balance between exploration & exploitation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Solutions?\n",
    "1. what's the best move?\n",
    "1. **time(step)-dependency**\n",
    "    - ensure the model can handle long time-series\n",
    "1. training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Time dependency (poblem 2)\n",
    "- the **reward** and future **state** depends on the current **state** and the **action** taken!\n",
    "    - $(s_{t+1}, r_{t}) = f(a_t, s_t)$\n",
    "    - e.g. for chess\n",
    "        - $s_t$ - state of the chessboard now (position of all pieces)\n",
    "        - $a_t$ - the move we make\n",
    "        - $s_{t+1}$ - chessboard in the next time step\n",
    "- our task is maximise the **total reward**\n",
    "    - $TR_T = \\sum_{t=1}^{t=T}r_t$\n",
    "    - **agent** learns to estimate the `value` which is an approximation of $TR$\n",
    "    - we can estimate both the value of state (`value` function) and action-state (`action value` function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is RL?\n",
    "- a study of how \"intelligent\" **agents** should take **actions** within an **environment** to collect maximum cummulative **reward**\n",
    "- sequential classification problem, where each decission (prediction) results in some reward\n",
    "    - we are trying to maximise the sum of these rewards\n",
    "- often defined as Markov Decission Process (MDP)\n",
    "    - $S$, $A$, $P_a(s'|s)$, $R_a(s'|s)$\n",
    "    - in this case, solutions are often based on concepts of dynammic programming (Bellman equation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Contextual Multi-Armed Bandits (CMAB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"https://diydilettante.files.wordpress.com/2011/07/the-office-pictures.jpg\">\n",
    "\n",
    "###### credits: Murder (The Office)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Contextual Multi-Armed Bandits (CMAB)\n",
    "<img src=\"https://slivkins.com/work/bandits-svc/MAB-2.jpg\">\n",
    "\n",
    "###### credits: Alex Slivkins - Microsoft Research Silicon Valley"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## CMAB vs RL?\n",
    "- simplification of the RL problem\n",
    "    - remove the long-term dependency\n",
    "    - the future **state** no longer depends on the current **state** and **action**\n",
    "        - and it is now called **context** ($c_t$)\n",
    "    - in RL:\n",
    "        - $(s_{t+1}, r_{t}) = f(a_t, s_t)$\n",
    "    - in CMAB:\n",
    "        - $r_{t} = f(a_t, c_t)$\n",
    "        - $c_{t+1}$ is independent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What are CMAB?\n",
    "- generalisation of classification problem\n",
    "- we don't have any training dataset, but can define **environment**\n",
    "    - we get context - $c_t$ (observation)\n",
    "    - we are asked to select one out of $N$ actions - $a_t$ (prediction)\n",
    "    - we receive reward - $r_t$\n",
    "- we collect data by interacting with **environment**\n",
    "    - and train our **agent** on this experience"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Agents\n",
    "- consist of:\n",
    "    - **oracle** - regression model that predicts the `reward` of each `action` based on `context`\n",
    "        - $\\hat r_{a, t} = o_{a}(c_t)$\n",
    "        - this can be generally any supervised regression model (linear regression, neural net)\n",
    "    - **policy** - decide on the `action` based on predictions from oracle\n",
    "        - $\\hat a_t = p(\\hat r_{a_1,t}, \\hat r_{a_2,t}, ..., \\hat r_{a_N,t})$\n",
    "        - we have a few general types that we will review now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## $\\epsilon$-Greedy (eps)\n",
    "- how does it work:\n",
    "    - select random action with probability $\\epsilon$\n",
    "    - otherwise, select the action with highest predicted reward\n",
    "- needs just oracle's point estimates!\n",
    "- very simple, but suboptimal\n",
    "    - will continue exploring forever\n",
    "    - there are extentions that try to alleviate - e.g. time-decay, adaptive greedy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Boltzman Exploration\n",
    "- how does it work:\n",
    "    - calculate probability of each action based on it's predicted reward        \n",
    "        - $P(a,t) = \\frac{exp(\\frac{\\hat r_{a, t}}{\\tau})}{\\sum_{i=1}^N exp(\\frac{\\hat r_{a, t}}{\\tau})}$\n",
    "    - sample with these probabilities as weights\n",
    "    - parameter $\\tau$ (temperature) - used to control the spread of softmax\n",
    "- needs just oracle's point estimates!\n",
    "- more flexible than **eps**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Upper Confidence Bound (UCB)\n",
    "- how does it work:\n",
    "    - combine estimated mean (point estimate) and variance of each action into UCB value\n",
    "    - $UCB_{a, t} = \\hat r_{a, t} + \\alpha \\frac{Var(\\hat r_{a, t})}{n_a}$\n",
    "    - $\\hat a_t = \\underset{a_t \\in \\{a_1, a_2, ..., a_N\\} }{\\operatorname{argmax}} UCB_{a,t}$\n",
    "    - parameter $\\alpha$ - used to control the amount of exploration\n",
    "- requires oracle to provide **variance** of the estimate as well!\n",
    "    - this is easy for linear regression\n",
    "    - not so much for non-parametric methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Thompson Sampling (TS)\n",
    "- how does it work:\n",
    "    - define reward distribution of each action using estimated mean (point estimate) and variance\n",
    "        - $D_{a, t} = \\mathcal{N}(\\hat r_{a, t}, \\alpha \\frac{Var(\\hat r_{a, t})}{n_a})$\n",
    "    - draw a sample ($\\hat r^{TS}_{a_1,t}$) from each distribution and select argmax\n",
    "        - $\\hat a_t = \\underset{a_t \\in \\{a_1, a_2, ..., a_N\\} }{\\operatorname{argmax} \\{\\hat r^{TS}_{a_1,t}, \\hat r^{TS}_{a_2,t}, ... \\hat r^{TS}_{a_N,t}\\}} $\n",
    "    - parameter $\\alpha$ - used to control the amount of exploration\n",
    "- in practise we just use normal, but you could potentially select arbitrary distribution\n",
    "- requires oracle to provide **variance** of the estimate as well!\n",
    "    - this is easy for linear regression\n",
    "    - not so much for non-parametric methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## CMAB summarised\n",
    "- for typical classification problems (independent observations)\n",
    "- define environment (with reward function) instead of collecting \"true\" labels\n",
    "- train regression models to map the context to reward (oracle)\n",
    "- utilise decission function to select action based on the expected rewards (policy)\n",
    "    - responsible for balancing exploration and exploitation\n",
    "- iteratively collect data and train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# TF-Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## More terms\n",
    "- **time_step** - single output from **environment** for **agent**\n",
    "    - `state (S)` - `reward (R)` - `discount` - `step type`\n",
    "    - e.g. state of chessboard + reward for previous action\n",
    "- **policy_step** - single output from **agent**\n",
    "    - `action (A)` - `policy info`\n",
    "- **epoch** - a group of timesteps\n",
    "    - e.g. one game of chess\n",
    "- **trajectory** - collected values of `S`, `A`, `R` from observed timesteps\n",
    "    - e.g. `namedtuple` of `numpy.arrays`\n",
    "    - collected during the **environment** - **state** iterations and used for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"www/tf_agents.png\">\n",
    "\n",
    "###### credits: [Inside TF-Agents](https://youtu.be/U7g7-Jzj9qo?t=1450)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## TensorFlow cheatsheet\n",
    "- [tf.cast](https://www.tensorflow.org/api_docs/python/tf/cast) - change data type (TensorFlow is very strict about `dtypes`)\n",
    "- [tf.math.bincount](https://www.tensorflow.org/api_docs/python/tf/math/bincount) - count occurence of each number from `0` to `maxlength`\n",
    "    - `tf.math.bincount([0, 0, 2, 3, 3, 5]) -> [2, 0, 1, 2, 0, 1]`\n",
    "- [tf.math.reduce_{max,min,mean,...}](https://www.tensorflow.org/api_docs/python/tf/math/reduce_sum) - instead of classical `sum`, `max`, ...\n",
    "- [tf.boolean_mask](https://www.tensorflow.org/api_docs/python/tf/boolean_mask) - filter data Tensor using boolean Tensor \n",
    "- [tf.where](https://www.tensorflow.org/api_docs/python/tf/where) - like numpy equivalent\n",
    "- [tf.one_hot](https://www.tensorflow.org/api_docs/python/tf/one_hot) - one-hot encoding of Tensor"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
