{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"../..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from typing import Optional\n",
    "\n",
    "from tf_agents.bandits.agents.lin_ucb_agent import LinearUCBAgent\n",
    "from tf_agents.bandits.environments.bandit_tf_environment import BanditTFEnvironment\n",
    "from tf_agents.drivers.dynamic_step_driver import DynamicStepDriver\n",
    "from tf_agents.policies.utils import get_num_actions_from_tensor_spec\n",
    "from tf_agents.replay_buffers.tf_uniform_replay_buffer import TFUniformReplayBuffer\n",
    "from tf_agents.specs import BoundedTensorSpec\n",
    "from tf_agents.specs.tensor_spec import TensorSpec\n",
    "import tf_agents.trajectories.time_step as ts\n",
    "from tf_agents.typing import types\n",
    "from tf_agents.utils import common, eager_utils\n",
    "\n",
    "from src.practise.utils import predict\n",
    "from src.practise.solution_metrics import RMSEMetric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCHSIZE = 1000\n",
    "DS_FOLDER = \"data/4_dataset/\"\n",
    "\n",
    "context = pd.read_csv(f\"{DS_FOLDER}user_context.csv\")\n",
    "ratings = pd.read_csv(\"data/4_dataset/reward_per_book.csv\")\n",
    "actions = ratings.columns[1:]\n",
    "\n",
    "train_df = context.merge(ratings, how=\"left\", on=\"user_id\")\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (\n",
    "        train_df.drop(ratings.columns, axis=1),\n",
    "        train_df[actions].astype(\"int32\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "NUM_EPOCHS = int(train_df.shape[0] / BATCHSIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_spec = BoundedTensorSpec(\n",
    "    shape=(), dtype=tf.int32, minimum=0, maximum=len(actions) - 1, name=\"action\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elements of TF-Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO BE FILLED!\n",
    "# in this part, you should prepare:\n",
    "#   - SimpleEnvironment - implementation of BanditTFEnvironment protocol\n",
    "\n",
    "\n",
    "class SimpleEnvironment(BanditTFEnvironment):\n",
    "    \"\"\"SimpleEnvironment for book recommendations.\n",
    "\n",
    "    Attributes:\n",
    "        labels: tf.Variable(shape=(batch_size, num_actions), dtype=tf.int32)\n",
    "            index of the optimal action\n",
    "        num_actions: tf.constant(dtype=tf.int32)\n",
    "            number of playable actions\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset: tf.data.Dataset,\n",
    "        batch_size: int,\n",
    "        action_spec: tf.TensorSpec,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataset: tf.data.Dataset\n",
    "                unbatched dataset with two elements - (features, labels)\n",
    "            batch_size: int\n",
    "                dataset batch size to be used\n",
    "            action_spec: discrete! BoundedTensorSpec\n",
    "                discrete bounded TensorSpec specifiyng possible actions,\n",
    "                it is used to extract number of actions and dtype of actions\n",
    "        \"\"\"\n",
    "        # this needs to be filled as well :)\n",
    "        super(SimpleEnvironment, self).__init__(\n",
    "            time_step_spec=None,\n",
    "            action_spec=None,\n",
    "            batch_size=None,\n",
    "            name=\"SimpleEnvironment\",\n",
    "        )\n",
    "\n",
    "    def _observe(self) -> types.NestedArray:\n",
    "        \"\"\"Collects another batch of features and labels and prepares time_step.\n",
    "        Updates current and previous labels.\n",
    "\n",
    "        Returns:\n",
    "            tf.Tensor(shape=(batch_size, num_features), dtype=self.time_step.obsevation.dtype)\n",
    "                context\n",
    "        \"\"\"\n",
    "        return tf.random.uniform([500, 101], dtype=tf.float64)\n",
    "\n",
    "    def _apply_action(self, action: types.NestedArray) -> types.Float:\n",
    "        \"\"\"Calculates rewards for current batch of actions.\n",
    "\n",
    "        Args:\n",
    "            action: tf.Tensor(shape=(batch_size, 1), dtype=self.action_spec.dtype)\n",
    "\n",
    "        Returns:\n",
    "            tf.Tensor(shape=(batch_size, 1), dtype=time_step_spec.reward.dtype)\n",
    "            Rewards for each played action.\n",
    "        \"\"\"\n",
    "        return tf.random.uniform([500], dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SimpleEnvironment(dataset=train_ds, batch_size=BATCHSIZE, action_spec=action_spec)\n",
    "\n",
    "agent = LinearUCBAgent(\n",
    "    env.time_step_spec(),\n",
    "    env.action_spec(),\n",
    "    tikhonov_weight=1,\n",
    "    alpha=1,\n",
    "    use_eigendecomp=True,\n",
    "    emit_policy_info=(\n",
    "        \"predicted_rewards_mean\",\n",
    "        \"predicted_rewards_optimistic\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = TFUniformReplayBuffer(\n",
    "    data_spec=agent.policy.trajectory_spec,\n",
    "    batch_size=BATCHSIZE,\n",
    "    max_length=5,\n",
    ")\n",
    "\n",
    "rmse = RMSEMetric(env.action_spec())\n",
    "\n",
    "replay_observer = [replay_buffer.add_batch, rmse]\n",
    "\n",
    "driver = DynamicStepDriver(\n",
    "    env=env,\n",
    "    policy=agent.collect_policy,\n",
    "    observers=replay_observer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_values = []\n",
    "for i in range(NUM_EPOCHS):\n",
    "\n",
    "    print(f\"\\rEpoch: {i+1}/{NUM_EPOCHS}\", end=\"\")\n",
    "\n",
    "    # environment - agent interactions\n",
    "    replay_buffer.clear()\n",
    "    _ = driver.run()\n",
    "    rmse_values.append(rmse.result().numpy())\n",
    "\n",
    "    # collect data and train\n",
    "    experience = replay_buffer.as_dataset(\n",
    "        sample_batch_size=BATCHSIZE, num_steps=1, single_deterministic_pass=True\n",
    "    )\n",
    "    for t in experience:\n",
    "        _ = agent.train(t[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(30, 10))\n",
    "fig.suptitle(\"RMSE value by action\")\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    vals = [r[i] for r in rmse_values]\n",
    "    ax.plot(vals)\n",
    "    ax.hlines(tf.math.reduce_mean(vals), xmin=0, xmax=NUM_EPOCHS, color=\"red\")\n",
    "    ax.title.set_text(actions[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scored_ds = train_ds.batch(1000).map(lambda x, y: predict(x, agent, env))\n",
    "preds = list(scored_ds)\n",
    "\n",
    "acts = tf.concat([i.action for i in preds], axis=0).numpy()\n",
    "\n",
    "scored_df = train_df.assign(\n",
    "    pred=acts,\n",
    ")\n",
    "\n",
    "score_rat = scored_df[actions.values]\n",
    "score_rat.columns = range(score_rat.shape[1])\n",
    "\n",
    "scored_df = scored_df.assign(rating=score_rat.idxmax(axis=1).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy\n",
    "(scored_df.rating == scored_df.pred).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix\n",
    "confmat = (\n",
    "    scored_df.groupby([\"rating\", \"pred\"], as_index=False)\n",
    "    .user_id.count()\n",
    "    .pivot_table(index=\"rating\", columns=\"pred\", values=\"user_id\", fill_value=0)\n",
    ")\n",
    "confmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# close rates\n",
    "confmat.values.diagonal() / confmat.sum()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c4d660e557e8b16ee4db5b68978562a6a0421a7ea0c415df0b2c7083e8bde969"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('mlprague')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
