{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"../..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from tf_agents.bandits.agents.lin_ucb_agent import LinearUCBAgent\n",
    "from tf_agents.bandits.agents.linear_thompson_sampling_agent import (\n",
    "    LinearThompsonSamplingAgent,\n",
    ")\n",
    "from tf_agents.bandits.environments.classification_environment import (\n",
    "    ClassificationBanditEnvironment,\n",
    ")\n",
    "from tf_agents.drivers.dynamic_step_driver import DynamicStepDriver\n",
    "from tf_agents.replay_buffers.tf_uniform_replay_buffer import TFUniformReplayBuffer\n",
    "from tf_agents.bandits.metrics.tf_metrics import RegretMetric\n",
    "\n",
    "from src.practise.utils import prep_reward_binary, predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCHSIZE = 1000\n",
    "DS_FOLDER = \"data/4_dataset/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = pd.read_csv(f\"{DS_FOLDER}user_context.csv\")\n",
    "ratings = pd.read_csv(\"data/4_dataset/reward_simple.csv\")\n",
    "\n",
    "train_df = context.merge(ratings, how=\"left\", on=\"user_id\")\n",
    "train_df.rating.fillna(0, inplace=True)\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (\n",
    "        train_df.drop([\"user_id\", \"rating\"], axis=1),\n",
    "        train_df.rating.astype(\"int32\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "NUM_EPOCHS = int(train_df.shape[0] / BATCHSIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rew_dist = prep_reward_binary(\n",
    "    rew_true_neg=1,\n",
    "    rew_false_neg=0,\n",
    "    rew_false_pos=0,\n",
    "    rew_true_pos=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elements of TF-Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO BE FILLED!\n",
    "# in this part, you should prepare:\n",
    "#   - ClassificationBanditEnvironment\n",
    "#   - some linear bandit (LinearUCBAgent or LinearThompsonSamplingAgent)\n",
    "env = ClassificationBanditEnvironment(train_ds, rew_dist, BATCHSIZE)\n",
    "\n",
    "agent = LinearUCBAgent(\n",
    "    env.time_step_spec(),\n",
    "    env.action_spec(),\n",
    "    tikhonov_weight=1,\n",
    "    alpha=1,\n",
    "    use_eigendecomp=True,\n",
    "    emit_policy_info=(\n",
    "        \"predicted_rewards_mean\",\n",
    "        \"predicted_rewards_optimistic\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your environment\n",
    "# ts = env.reset()\n",
    "# for i in range(5):\n",
    "#     act = agent.policy.action(ts)\n",
    "#     env.step(act.action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO BE FILLED!\n",
    "# in this part, you should prepare:\n",
    "#   - RegretMetric\n",
    "optimal_reward = lambda x: env.compute_optimal_reward()\n",
    "\n",
    "regret = RegretMetric(optimal_reward, name=\"regret\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO BE FILLED!\n",
    "# in this part, you should prepare:\n",
    "#   - TFUniformReplayBuffer\n",
    "#   - DynamicStepDriver (that will use the replay buffer)\n",
    "\n",
    "replay_buffer = TFUniformReplayBuffer(\n",
    "    data_spec=agent.policy.trajectory_spec,\n",
    "    batch_size=BATCHSIZE,\n",
    "    max_length=5,\n",
    ")\n",
    "\n",
    "replay_observer = [replay_buffer.add_batch, regret]\n",
    "\n",
    "driver = DynamicStepDriver(\n",
    "    env=env,\n",
    "    policy=agent.collect_policy,\n",
    "    observers=replay_observer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regret_values = []\n",
    "for i in range(NUM_EPOCHS):\n",
    "\n",
    "    print(f\"\\rEpoch: {i+1}/{NUM_EPOCHS}\", end=\"\")\n",
    "\n",
    "    # environment - agent interactions\n",
    "    replay_buffer.clear()\n",
    "    _ = driver.run()\n",
    "    regret_values.append(regret.result())\n",
    "\n",
    "    # collect data and train\n",
    "    experience = replay_buffer.as_dataset(\n",
    "        sample_batch_size=BATCHSIZE, num_steps=1, single_deterministic_pass=True\n",
    "    )\n",
    "    for t in experience:\n",
    "        _ = agent.train(t[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspired by https://www.tensorflow.org/agents/tutorials/per_arm_bandits_tutorial#defining_the_regret_metric\n",
    "plt.plot(regret_values)\n",
    "plt.hlines(tf.math.reduce_mean(regret_values), xmin=0, xmax=NUM_EPOCHS, color=\"orange\")\n",
    "plt.title(\"Regret of Linear Agent\")\n",
    "plt.xlabel(\"Number of Iterations\")\n",
    "_ = plt.ylabel(\"Average Regret\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scored_ds = train_ds.batch(1000).map(lambda x, y: predict(x, agent, env))\n",
    "preds = list(scored_ds)\n",
    "\n",
    "acts = tf.concat([i.action for i in preds], axis=0).numpy()\n",
    "rews = tf.concat([i.info.predicted_rewards_mean[:, 1] for i in preds], axis=0).numpy()\n",
    "\n",
    "scored_df = train_df.assign(pred=acts, rew=rews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy\n",
    "(scored_df.rating == scored_df.pred).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix\n",
    "(\n",
    "    scored_df.groupby([\"rating\", \"pred\"], as_index=False)\n",
    "    .user_id.count()\n",
    "    .pivot(index=\"rating\", columns=\"pred\", values=\"user_id\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scored_df[[\"rating\", \"pred\", \"rew\"]].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minmax = lambda x: (x - x.min()) / (x.max() - x.min())\n",
    "\n",
    "plot_df = scored_df.sort_values(\"rew\").assign(ix=range(scored_df.shape[0]))\n",
    "\n",
    "plt.scatter(plot_df.ix, plot_df.rating + 0.1 * np.random.randn(plot_df.shape[0]))\n",
    "plt.plot(plot_df.ix, minmax(plot_df.rew), color=\"red\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c4d660e557e8b16ee4db5b68978562a6a0421a7ea0c415df0b2c7083e8bde969"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('mlprague')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
