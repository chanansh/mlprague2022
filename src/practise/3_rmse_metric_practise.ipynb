{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"../..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from tf_agents.bandits.agents.lin_ucb_agent import LinearUCBAgent\n",
    "from tf_agents.bandits.environments.classification_environment import (\n",
    "    ClassificationBanditEnvironment,\n",
    ")\n",
    "from tf_agents.drivers.dynamic_step_driver import DynamicStepDriver\n",
    "from tf_agents.replay_buffers.tf_uniform_replay_buffer import TFUniformReplayBuffer\n",
    "\n",
    "from tf_agents.utils.common import create_variable\n",
    "from tf_agents.metrics import tf_metric\n",
    "from tf_agents.policies.utils import get_num_actions_from_tensor_spec\n",
    "from tf_agents.specs import BoundedTensorSpec\n",
    "from tf_agents.trajectories.trajectory import Trajectory\n",
    "\n",
    "from src.practise.utils import prep_reward_binary, predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCHSIZE = 1000\n",
    "DS_FOLDER = \"data/4_dataset/\"\n",
    "\n",
    "context = pd.read_csv(f\"{DS_FOLDER}user_context.csv\")\n",
    "ratings = pd.read_csv(\"data/4_dataset/reward_simple.csv\")\n",
    "\n",
    "train_df = context.merge(ratings, how=\"left\", on=\"user_id\")\n",
    "train_df.rating.fillna(0, inplace=True)\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (\n",
    "        train_df.drop([\"user_id\", \"rating\"], axis=1),\n",
    "        train_df.rating.astype(\"int32\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "NUM_EPOCHS = int(train_df.shape[0] / BATCHSIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rew_dist = prep_reward_binary(\n",
    "    rew_true_neg=1,\n",
    "    rew_false_neg=0,\n",
    "    rew_false_pos=0,\n",
    "    rew_true_pos=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elements of TF-Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = ClassificationBanditEnvironment(train_ds, rew_dist, BATCHSIZE)\n",
    "\n",
    "agent = LinearUCBAgent(\n",
    "    env.time_step_spec(),\n",
    "    env.action_spec(),\n",
    "    tikhonov_weight=1,\n",
    "    alpha=1,\n",
    "    use_eigendecomp=True,\n",
    "    emit_policy_info=(\n",
    "        \"predicted_rewards_mean\",\n",
    "        \"predicted_rewards_optimistic\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO BE FILLED!\n",
    "# in this part, you should prepare:\n",
    "#   - RMSEMetric\n",
    "\n",
    "\n",
    "class RMSEMetric(tf_metric.TFStepMetric):\n",
    "    \"\"\"RMSE metric implementation for TF agents\n",
    "\n",
    "    Attributes:\n",
    "        action_count: int\n",
    "            how many possible actions are there\n",
    "        action_spec: discrete! BoundedTensorSpec\n",
    "                discrete bounded TensorSpec specifiyng possible actions\n",
    "        rmse: tf.Variable(shape=(action_count), dtype=tf.float64)\n",
    "            rmse value of each action on current batch\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        action_spec: BoundedTensorSpec,\n",
    "        name: str = \"RMSEMetric\",\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            action_spec: discrete! BoundedTensorSpec\n",
    "                discrete bounded TensorSpec specifiyng possible actions,\n",
    "                it is used to extract number of actions and dtype of actions\n",
    "            type: str\n",
    "                which field of policy info should be used as prediction,\n",
    "                possible values are [predicted_rewards_mean, predicted_rewards_optimistic]\n",
    "            name: optional, name of the metric\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset the metric.\n",
    "\n",
    "        Reset removes all calculated values.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, t: Trajectory) -> Trajectory:\n",
    "        \"\"\"Accumulates statistics from the provided trajectory.\n",
    "\n",
    "        Each call expects a `trajectory` based on a new batch of data.\n",
    "        Each `trajectory` is expected to contain nonempty field `policy_info.predicted_rewards_mean`,\n",
    "\n",
    "        Args:\n",
    "            t: tf_agents.trajectories.trajectory.Trajectory\n",
    "                trajectory to use for metric calculation\n",
    "\n",
    "        Returns:\n",
    "            The same trajectory that was passed as an argument\n",
    "        \"\"\"\n",
    "        return t\n",
    "\n",
    "    def result(self) -> tf.Tensor:\n",
    "        \"\"\"Provides the RMSE value\n",
    "\n",
    "        Returns:\n",
    "            tf.Tensor(shape=(action_count), dtype=float64) with final value of the metric\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = TFUniformReplayBuffer(\n",
    "    data_spec=agent.policy.trajectory_spec,\n",
    "    batch_size=BATCHSIZE,\n",
    "    max_length=5,\n",
    ")\n",
    "\n",
    "rmse = RMSEMetric(env.action_spec())\n",
    "\n",
    "replay_observer = [replay_buffer.add_batch, rmse]\n",
    "\n",
    "driver = DynamicStepDriver(\n",
    "    env=env,\n",
    "    policy=agent.collect_policy,\n",
    "    observers=replay_observer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_values = []\n",
    "for i in range(NUM_EPOCHS):\n",
    "\n",
    "    print(f\"\\rEpoch: {i+1}/{NUM_EPOCHS}\", end=\"\")\n",
    "\n",
    "    # environment - agent interactions\n",
    "    replay_buffer.clear()\n",
    "    _ = driver.run()\n",
    "    rmse_values.append(rmse.result().numpy())\n",
    "\n",
    "    # collect data and train\n",
    "    experience = replay_buffer.as_dataset(\n",
    "        sample_batch_size=BATCHSIZE, num_steps=1, single_deterministic_pass=True\n",
    "    )\n",
    "    for t in experience:\n",
    "        _ = agent.train(t[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(rmse_values)\n",
    "plt.hlines(\n",
    "    tf.math.reduce_mean(rmse_values, axis=0), xmin=0, xmax=NUM_EPOCHS, color=\"red\"\n",
    ")\n",
    "plt.title(\"RMSE value by action\")\n",
    "plt.xlabel(\"Number of Iterations\")\n",
    "_ = plt.ylabel(\"RMSE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w0 = 1\n",
    "w1 = 1\n",
    "rmse_vals_w = [i / [w0, w1] for i in rmse_values]\n",
    "plt.plot(rmse_vals_w)\n",
    "plt.hlines(\n",
    "    tf.math.reduce_mean(rmse_vals_w, axis=0), xmin=0, xmax=NUM_EPOCHS, color=\"red\"\n",
    ")\n",
    "plt.title(\"RMSE value by action\")\n",
    "plt.xlabel(\"Number of Iterations\")\n",
    "_ = plt.ylabel(\"RMSE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scored_ds = train_ds.batch(1000).map(lambda x, y: predict(x, agent, env))\n",
    "preds = list(scored_ds)\n",
    "\n",
    "acts = tf.concat([i.action for i in preds], axis=0).numpy()\n",
    "rews = tf.concat([i.info.predicted_rewards_mean[:, 1] for i in preds], axis=0).numpy()\n",
    "\n",
    "scored_df = train_df.assign(pred=acts, rew=rews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy\n",
    "(scored_df.rating == scored_df.pred).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix\n",
    "(\n",
    "    scored_df.groupby([\"rating\", \"pred\"], as_index=False)\n",
    "    .user_id.count()\n",
    "    .pivot(index=\"rating\", columns=\"pred\", values=\"user_id\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scored_df[[\"rating\", \"pred\", \"rew\"]].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minmax = lambda x: (x - x.min()) / (x.max() - x.min())\n",
    "\n",
    "plot_df = scored_df.sort_values(\"rew\").assign(ix=range(scored_df.shape[0]))\n",
    "\n",
    "plt.scatter(plot_df.ix, plot_df.rating + 0.1 * np.random.randn(plot_df.shape[0]))\n",
    "plt.plot(plot_df.ix, minmax(plot_df.rew), color=\"red\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c4d660e557e8b16ee4db5b68978562a6a0421a7ea0c415df0b2c7083e8bde969"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('mlprague')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
